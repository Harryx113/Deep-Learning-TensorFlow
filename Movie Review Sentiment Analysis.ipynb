{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = keras.datasets.imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_label), (test_data, test_label) = data.load_data(num_words=880000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = data.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {k:(v+3) for k, v in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index['<PAD>'] = 0\n",
    "word_index['<START>'] = 1\n",
    "word_index['<UNK>'] = 2\n",
    "word_index['<UNUSED>'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> please give this one a miss br br kristy swanson and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite lacklustre so all you madison fans give this a miss\n"
     ]
    }
   ],
   "source": [
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "print(decode_review(test_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "603 230\n"
     ]
    }
   ],
   "source": [
    "# different size\n",
    "print(len(test_data[2]), len(test_data[20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess to the same size\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data, value=word_index['<PAD>'], padding='post', maxlen=250)\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data, value=word_index['<PAD>'], padding='post', maxlen=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 250\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data[2]), len(test_data[20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(880000, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 16)          14080000  \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 14,080,289\n",
      "Trainable params: 14,080,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# embedding layer: group words based on similarity\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set\n",
    "x_val = train_data[:10000]\n",
    "y_val = train_label[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set\n",
    "x_train = train_data[10000:]\n",
    "y_train = train_label[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "15000/15000 [==============================] - 5s 306us/sample - loss: 0.6920 - accuracy: 0.6087 - val_loss: 0.6901 - val_accuracy: 0.6771\n",
      "Epoch 2/40\n",
      "15000/15000 [==============================] - 4s 278us/sample - loss: 0.6864 - accuracy: 0.7339 - val_loss: 0.6825 - val_accuracy: 0.7402\n",
      "Epoch 3/40\n",
      "15000/15000 [==============================] - 5s 337us/sample - loss: 0.6741 - accuracy: 0.7676 - val_loss: 0.6672 - val_accuracy: 0.7633\n",
      "Epoch 4/40\n",
      "15000/15000 [==============================] - 5s 304us/sample - loss: 0.6517 - accuracy: 0.7845 - val_loss: 0.6418 - val_accuracy: 0.7661\n",
      "Epoch 5/40\n",
      "15000/15000 [==============================] - 4s 276us/sample - loss: 0.6170 - accuracy: 0.8103 - val_loss: 0.6064 - val_accuracy: 0.7898\n",
      "Epoch 6/40\n",
      "15000/15000 [==============================] - 4s 255us/sample - loss: 0.5717 - accuracy: 0.8298 - val_loss: 0.5643 - val_accuracy: 0.8100\n",
      "Epoch 7/40\n",
      "15000/15000 [==============================] - 4s 279us/sample - loss: 0.5201 - accuracy: 0.8433 - val_loss: 0.5189 - val_accuracy: 0.8235\n",
      "Epoch 8/40\n",
      "15000/15000 [==============================] - 4s 273us/sample - loss: 0.4678 - accuracy: 0.8623 - val_loss: 0.4769 - val_accuracy: 0.8334\n",
      "Epoch 9/40\n",
      "15000/15000 [==============================] - 4s 269us/sample - loss: 0.4188 - accuracy: 0.8774 - val_loss: 0.4390 - val_accuracy: 0.8444\n",
      "Epoch 10/40\n",
      "15000/15000 [==============================] - 4s 282us/sample - loss: 0.3748 - accuracy: 0.8925 - val_loss: 0.4073 - val_accuracy: 0.8531\n",
      "Epoch 11/40\n",
      "15000/15000 [==============================] - 4s 277us/sample - loss: 0.3367 - accuracy: 0.9017 - val_loss: 0.3811 - val_accuracy: 0.8615\n",
      "Epoch 12/40\n",
      "15000/15000 [==============================] - 4s 272us/sample - loss: 0.3042 - accuracy: 0.9089 - val_loss: 0.3605 - val_accuracy: 0.8671\n",
      "Epoch 13/40\n",
      "15000/15000 [==============================] - 4s 271us/sample - loss: 0.2770 - accuracy: 0.9159 - val_loss: 0.3426 - val_accuracy: 0.8714\n",
      "Epoch 14/40\n",
      "15000/15000 [==============================] - 4s 270us/sample - loss: 0.2526 - accuracy: 0.9246 - val_loss: 0.3291 - val_accuracy: 0.8753\n",
      "Epoch 15/40\n",
      "15000/15000 [==============================] - 4s 272us/sample - loss: 0.2318 - accuracy: 0.9316 - val_loss: 0.3186 - val_accuracy: 0.8784\n",
      "Epoch 16/40\n",
      "15000/15000 [==============================] - 4s 271us/sample - loss: 0.2132 - accuracy: 0.9374 - val_loss: 0.3090 - val_accuracy: 0.8786\n",
      "Epoch 17/40\n",
      "15000/15000 [==============================] - 4s 271us/sample - loss: 0.1963 - accuracy: 0.9434 - val_loss: 0.3018 - val_accuracy: 0.8827\n",
      "Epoch 18/40\n",
      "15000/15000 [==============================] - 4s 270us/sample - loss: 0.1813 - accuracy: 0.9485 - val_loss: 0.2953 - val_accuracy: 0.8825\n",
      "Epoch 19/40\n",
      "15000/15000 [==============================] - 4s 275us/sample - loss: 0.1677 - accuracy: 0.9531 - val_loss: 0.2901 - val_accuracy: 0.8849\n",
      "Epoch 20/40\n",
      "15000/15000 [==============================] - 4s 293us/sample - loss: 0.1557 - accuracy: 0.9584 - val_loss: 0.2866 - val_accuracy: 0.8857\n",
      "Epoch 21/40\n",
      "15000/15000 [==============================] - 4s 275us/sample - loss: 0.1440 - accuracy: 0.9621 - val_loss: 0.2830 - val_accuracy: 0.8864\n",
      "Epoch 22/40\n",
      "15000/15000 [==============================] - 4s 289us/sample - loss: 0.1341 - accuracy: 0.9659 - val_loss: 0.2808 - val_accuracy: 0.8868\n",
      "Epoch 23/40\n",
      "15000/15000 [==============================] - 4s 287us/sample - loss: 0.1243 - accuracy: 0.9698 - val_loss: 0.2802 - val_accuracy: 0.8866\n",
      "Epoch 24/40\n",
      "15000/15000 [==============================] - 5s 339us/sample - loss: 0.1160 - accuracy: 0.9724 - val_loss: 0.2775 - val_accuracy: 0.8880\n",
      "Epoch 25/40\n",
      "15000/15000 [==============================] - 4s 296us/sample - loss: 0.1079 - accuracy: 0.9757 - val_loss: 0.2765 - val_accuracy: 0.8882\n",
      "Epoch 26/40\n",
      "15000/15000 [==============================] - 4s 294us/sample - loss: 0.1005 - accuracy: 0.9784 - val_loss: 0.2772 - val_accuracy: 0.8884\n",
      "Epoch 27/40\n",
      "15000/15000 [==============================] - 4s 295us/sample - loss: 0.0939 - accuracy: 0.9797 - val_loss: 0.2766 - val_accuracy: 0.8884\n",
      "Epoch 28/40\n",
      "15000/15000 [==============================] - 4s 296us/sample - loss: 0.0876 - accuracy: 0.9821 - val_loss: 0.2759 - val_accuracy: 0.8891\n",
      "Epoch 29/40\n",
      "15000/15000 [==============================] - 4s 296us/sample - loss: 0.0821 - accuracy: 0.9840 - val_loss: 0.2793 - val_accuracy: 0.8881\n",
      "Epoch 30/40\n",
      "15000/15000 [==============================] - 4s 296us/sample - loss: 0.0770 - accuracy: 0.9845 - val_loss: 0.2778 - val_accuracy: 0.8896\n",
      "Epoch 31/40\n",
      "15000/15000 [==============================] - 4s 294us/sample - loss: 0.0716 - accuracy: 0.9868 - val_loss: 0.2784 - val_accuracy: 0.8898\n",
      "Epoch 32/40\n",
      "15000/15000 [==============================] - 5s 339us/sample - loss: 0.0670 - accuracy: 0.9878 - val_loss: 0.2799 - val_accuracy: 0.8906\n",
      "Epoch 33/40\n",
      "15000/15000 [==============================] - 5s 314us/sample - loss: 0.0627 - accuracy: 0.9885 - val_loss: 0.2813 - val_accuracy: 0.8888\n",
      "Epoch 34/40\n",
      "15000/15000 [==============================] - 5s 310us/sample - loss: 0.0589 - accuracy: 0.9899 - val_loss: 0.2830 - val_accuracy: 0.8890\n",
      "Epoch 35/40\n",
      "15000/15000 [==============================] - 5s 326us/sample - loss: 0.0557 - accuracy: 0.9904 - val_loss: 0.2850 - val_accuracy: 0.8894\n",
      "Epoch 36/40\n",
      "15000/15000 [==============================] - 5s 312us/sample - loss: 0.0520 - accuracy: 0.9916 - val_loss: 0.2872 - val_accuracy: 0.8889\n",
      "Epoch 37/40\n",
      "15000/15000 [==============================] - 4s 296us/sample - loss: 0.0487 - accuracy: 0.9924 - val_loss: 0.2892 - val_accuracy: 0.8893\n",
      "Epoch 38/40\n",
      "15000/15000 [==============================] - 5s 303us/sample - loss: 0.0459 - accuracy: 0.9935 - val_loss: 0.2929 - val_accuracy: 0.8878\n",
      "Epoch 39/40\n",
      "15000/15000 [==============================] - 4s 297us/sample - loss: 0.0437 - accuracy: 0.9935 - val_loss: 0.2933 - val_accuracy: 0.8884\n",
      "Epoch 40/40\n",
      "15000/15000 [==============================] - 5s 301us/sample - loss: 0.0407 - accuracy: 0.9942 - val_loss: 0.2964 - val_accuracy: 0.8869\n"
     ]
    }
   ],
   "source": [
    "fitModel = model.fit(x_train, y_train, epochs=40, batch_size=512, validation_data=(x_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 40us/sample - loss: 0.3290 - accuracy: 0.8728\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: \n",
      "<START> please give this one a miss br br kristy swanson and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite lacklustre so all you madison fans give this a miss <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Prediction: [0.]\n",
      "Actual: 0\n",
      "[0.328954838271141, 0.87276]\n"
     ]
    }
   ],
   "source": [
    "test_review = test_data[0]\n",
    "predict = model.predict(test_review)\n",
    "print('Review: ')\n",
    "print(decode_review(test_review))\n",
    "print('Prediction: ' + str(predict[0]))\n",
    "print('Actual: ' + str(test_label[0]))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_encode(s):\n",
    "    encoded = [1]\n",
    "    for word in s:\n",
    "        if word.lower() in word_index:\n",
    "            encoded.append(word_index[word.lower()])\n",
    "        else:\n",
    "            encoded.append(2)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We begin by giving thanks to the local publicist who was swell enough to arrange a press preview ofÊThe IrishmanÊin an auditorium to my liking. (Reading Cinemas Grossmont #5.) That said, it angers me to no end that I will be one of just a few thousand Americans afforded a shot at seeing a film directed by Marvel-denier and ruler of the cinematic universe, Martin Scorsese, in the way it was intended. Or was it? Martin Scorsese shotÊRaging BullÊin black-and-white to protest what he understood to be a crisis in unstable color film stock. He was also the first to decry the multiplexing of American single screens in the name of staggered showtimes. Knowing that greater numbers of viewers were watching films on home video, he waited until 1991 Ñ when TV screens were big enough and viewers finally tolerant enough to deal with a letterboxed image Ñ to at last shoot a picture in Panavision. No one in my lifetime has done more to preserve film and encourage the theatrical life cycle of motion pictures than Scorsese. So what the fuck is he doing in bed with Netflix? Netflix is to theatrical distribution what Trump is to detente. It is a television channel, and as such, its goal is to keep people at home microwaving corn, not purchasing it freshly popped at a concession stand. During the last gasp of 35mm exhibition, indie films would play the Gaslamp for a week or two in advance of their VOD release. These last minute bookings functioned more as commercial reminders for films coming soon to a home theatre (or Red Box) near you than actual incentives to leave the living room. Netflix will release certain titles to theatres only to remind people to subscribe to their streaming service or to qualify the pics for prizes. Note that for its New York and Los Angeles releases, the studio, looking to suck up to voting members, bookedÊThe IrishmanÊin a pair of single-screen palaces. IsnÕt it ironic that a man who rightfully dubbed effects-driven Marvel Movies Òtheme park ridesÓ and Ònot cinemaÓ had every studio gate slammed in his face when he insistedÊThe IrishmanÊcould not be made without costly de-aging VFX? I will concede that the technology was as essential as it was effective, seamlessly souping up credibility in a manner that leaves latex and Max Factor in the dust. But thatÕs not enough to cover for Scorsese. He knew all along that the majority of viewers would come to the film on television, so he shot accordingly. A betting man would wager that well over half the picture was composed in TV-safe closeups and reverse angle shots of well-paid actors talking, the impact of which would not be lost on a flatscreen. I liked it better when Scorsese, not the medium, dictated shot size. The film is not without its flashes of brilliance, many of which involve Al PacinoÕs Jimmy Hoffa. With his advancing hairline, Hoffa goes and gets his Irish up over a little matter at a union meeting. In no time, youÕre laughing harder than when watching Rob ÒMad MaxÓ Reiner field a call duringÊThe Equalizer. And why let news of the death of J.F.K. come between a covetous butcher and his ice cream sundae? Having never before worked with Scorsese, PacinoÕs fresh take on an old teamster elevates the film above its small screen trappings. Of the old guard, only Harvey Keitel is left with precious little to do. One might have hoped that a post-#MeToo Scorsese might have carved out some space in his picture for the womenfolk. Alas, women are not a part of ScorseseÕs lexicography, never have been. The wives are a pair of cigarette puffing props who are not allowed to smoke on the car trip that provides the film its structure. (DidnÕt Marty see Linda CardelliniÕs Dolores inÊGreen Book? He could have learned a lot from a Farrelly brother.) The other woman, SheeranÕs daughter Peggy (Lucy Gallina aging into Anna Paquin) observes at a very early age the type of monster her old man is. His attempt to be a good father by roughing up her boss was better played with HenryÕs dad and the pizza oven inÊGoodfellas. Screenwriter Steve Zailian affords Peggy but one line of dialogue, as if seething silence alone were commentary enough to build and sustain a character. (What can you expect from a man who couldnÕt figure out any other way of getting a little girl to stand out in a 195 minute black-and-white Holocaust drama than by putting her in a colorized red dress?) Some are saying the filmÕs curtain shot is the most powerful in ScorseseÕs career, but IÕm not buying it.ÊCasinoÊis ostensibly a wall-to-wall rollick until it reaches the point ofÊit ainÕt funny no more. The whacking and subsequent desert deposition of Nicky and Frank updates the time-honored Òcrime doesnÕt payÓ dictum with the force of an unglamorized attention-grabbing twist of the neck. Powerful moment #2 can be found inÊThe Irishman. A flashback to WWII finds Lt. Sheeran forcing a pair of Nazis to dig a hole that, in the time it takes to fire off a few rounds, becomes their eternal resting place. Little did the soldier know at the time, but this Òfollow orders and youÕre rewardedÓ mentality was preparing him for a career in the mob. But when it comes to a resounding moment of curtain-ringing restitution, nothing inÊThe IrishmanÊcomes close to touching HenryÕs comeuppance inÊGoodfellas. The punishment there is twofold: first, he can sin no more; and second, his jail cell is a traditional single-family suburban home of the white picket fence, two-car garage variety,ÊakaÊthe American dream. So what exactly is it aboutÊThe IrishmanÊthat left a stain of betrayal? Sheeran comes closer to granite-headed Jake LaMotta than any of MartyÕs previous mobsters. The key to both menÕs success was their inability to feel. IÕve felt enough. It didnÕt take a genius to see that television was finally poised to win the war against movies. I just didnÕt expect my teacher to be leading the charge against theatrical exhibition. For enjoyable entertainment, you could do a lot worse. You could revisitÊThe Departed. This time, the directorÕs biggest triumph was picking up a hefty paycheck for what is for all intents and purposes a $200 million TV movie. And the Emmy goes to\n",
      "[[20531     4     2     2     2     2 51860    19     4  1147     7    35\n",
      "      2     2  1009     7     4  3307   976   561   241    70    30   258\n",
      "      2 19639     6  2763     8  3018   659  8886     2  5780     6  2162\n",
      "      7  3970     8  3302     6  2944    15    11     4    58    12   304\n",
      "      8   968   125     6   171  7366   461    68  5646 19113   273   117\n",
      "    122     4  1631   124    33     4    58    21    14     2  4004     5\n",
      "      2     2  5695    16  6083    90    18     6   611    11     4  3024\n",
      "     21    54    12   266     8     6 14644   561     7     2 80568   164\n",
      "      2     2   491     8  1301     2 14699     2     4  4982    50     9\n",
      "  37712    86    29    70  3074    57    53     5   333    27  2829  2768\n",
      "      9     6  2040     2  6371   344     7     4   428 27157  8740     2\n",
      "   8108     2   298   925    38    51   618     9    12     2     2   317\n",
      "      6 21301     7  5465     2   266  2439     8     2  3273     2    74\n",
      "    101     7     2   960 11680     4  1317     8   199     2  1023    16\n",
      "     68  5150     8   235     2   421   195    12     2   193     6  1262\n",
      "      8    67    15   699    16   417 20315     8  1176     4   325   429\n",
      "    102    13    43     2   535    61  1750     8    30   971     4  2909\n",
      "    429  2253 15041    18   737   722    25   100    81     6   176   433\n",
      "     25   100     2 11487    14    58     4     2  1126  3823    16  3613\n",
      "     56     6 15330  7939    18    51     9    18    32 17272     5  4935\n",
      "      6  6305  1431   248    20     5     4  8693   271     8]]\n",
      "[0.9889557]\n"
     ]
    }
   ],
   "source": [
    "with open('Irishman.txt', encoding ='ISO-8859-1') as f:\n",
    "    for line in f.readlines():\n",
    "        nline = line.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        encode = review_encode(nline)\n",
    "        encode = keras.preprocessing.sequence.pad_sequences([encode], value=word_index['<PAD>'], padding='post', maxlen=250)\n",
    "        predict = model.predict(encode)\n",
    "        print(line)\n",
    "        print(encode)\n",
    "        print(predict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Winchester Mystery House in San Jose, California, is the ultimate horror movie locationÑa vast, four-story mansion constructed in crazy-quilt fashion from 1883 to 1922 by Sarah Winchester, heir to the gun manufacturing fortune. It comes with its own legend too, that Winchester built each room to mollify the spirit of a person killed by one of her company's weapons. That premise speaks to our current gun crisis, but directors Michael and Peter Spierig, reworking a script by Tom Vaughan, bring little passion or eloquence to the dialogue, frustrating such capable actors as Helen Mirren (playing Winchester) and Jason Clarke (as a laudanum-addicted doctor sent by the company board to assess Mrs. Winchester's sanity). The scares are standard haunted-house stuff; whenever someone in a movie like this starts fooling around with a hinged mirror, grab your armrests.\n",
      "[[    1     4  5829   736   313    11  2614  7139  2642     9     4  2095\n",
      "    189    20     2  4311     2  3025  4481    11     2  1597    39     2\n",
      "      8 11065    34  2695  5829  9077     8     4  1056 22456  3200    12\n",
      "    266    19    94   205  1795    99    15  5829  2171   257   673     8\n",
      "  66066     4  1103     7     6   415   556    34    31     7    41 64398\n",
      "   2872    15   863  2466     8   263  2028  1056  3112    21   907   488\n",
      "      5   826     2 11736     6   229    34   827 28778   721   117  1797\n",
      "     42 27359     8     4   414  5129   141  2250   156    17  3177 18021\n",
      "    395  5829     5  1655  5732    17     6     2  1042  1412    34     4\n",
      "   1169  2089     8 31735  2015     2  7617     4  2629    26  1273     2\n",
      "    538  1945   294    11     6    20    40    14   517 13457   187    19\n",
      "      6 43678  2915  4003   129     2     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0]]\n",
      "[0.0891126]\n"
     ]
    }
   ],
   "source": [
    "with open('Winchester.txt', encoding ='ISO-8859-1') as f:\n",
    "    for line in f.readlines():\n",
    "        nline = line.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        encode = review_encode(nline)\n",
    "        encode = keras.preprocessing.sequence.pad_sequences([encode], value=word_index['<PAD>'], padding='post', maxlen=250)\n",
    "        predict = model.predict(encode)\n",
    "        print(line)\n",
    "        print(encode)\n",
    "        print(predict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(file):\n",
    "    with open(file, encoding ='ISO-8859-1') as f:\n",
    "        for line in f.readlines():\n",
    "            nline = line.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "            encode = review_encode(nline)\n",
    "            encode = keras.preprocessing.sequence.pad_sequences([encode], value=word_index['<PAD>'], padding='post', maxlen=250)\n",
    "            predict = model.predict(encode)\n",
    "            print(line)\n",
    "            print(encode)\n",
    "            print(predict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Parasite\" moves effortlessly between capitalist parable, horror film and situation comedy as the natures of the two families are revealed: The Kims, grasping and greedy yet unfairly stigmatized by poverty (even in their newly bought clothes, they give off an odor that can't be masked), and the Parks, harmless and well-intentioned yet coasting through life on the backs of others. Who's the parasite, then? So recognizable is this picture of society that you may forget you aren't watching an American movie. (The spell breaks now and then when Mrs. Park tosses off an English catchphrase to prove her internationalist bona-fides.)\n",
      "[[    1 16422  1102  8233   200 11242 12214   189    22     5   904   212\n",
      "     17     4 18035     7     4   107  2166    26  2029     4     2 17641\n",
      "      5  4636   246  9996     2    34  3460    60    11    68  4700  1247\n",
      "   1649    36   202   125    35 43801    15  2488    30  7566     5     4\n",
      "   8664  5623     5     2   246 35620   143   113    23     4  5462     7\n",
      "    409 14125     4 16422    95    38  5710     9    14   431     7   926\n",
      "     15    25   203   859    25 38051   149    35   298    20     4  3341\n",
      "   2010   150     5    95    54  2015  1547 12623   125    35   631 19946\n",
      "      8  1970    41 54858     2     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0]]\n",
      "[0.64190775]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(sentiment('Parasite.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't think of any reason to leave a bad review for this movie. It is fantastic.\n",
      "[[   1   13 2488  104    7  101  282    8  563    6   78  733   18   14\n",
      "    20   12    9  777    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "[0.389409]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(sentiment('test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
